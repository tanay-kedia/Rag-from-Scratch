<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Interactive RAG Explainer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Warm Neutrals (Stone, Slate, Cyan) -->
    <!-- Application Structure Plan: A single-page scrolling application with a sticky top navigation bar. The structure is thematic, designed for a logical learning flow: 1. Introduction (What is RAG?), 2. The Pipeline (Interactive diagram), 3. RAG Simulator (NEW - Gemini-powered interactive demo), 4. RAG vs. Fine-Tuning (Chart), 5. Code Deep Dive (with NEW Gemini-powered explanations), 6. Knowledge Check (Quiz). This structure builds concepts progressively and now includes hands-on, AI-powered tools for deeper understanding. -->
    <!-- Visualization & Content Choices: 
        - RAG Pipeline Diagram: Goal=Organize/Inform, Method=HTML/CSS/JS interactive diagram.
        - RAG Simulator: Goal=Demonstrate, Method=Interactive HTML form calling Gemini API. Justification=Allows users to experience the RAG flow with their own data, making the concept tangible.
        - RAG vs. Fine-Tuning Chart: Goal=Compare, Method=Chart.js Bar Chart.
        - Code Viewer with AI Explainer: Goal=Inform/Explain, Method=Tabbed HTML with buttons calling Gemini API. Justification=Provides dynamic, in-depth code explanations on demand.
        - Quiz: Goal=Assess, Method=HTML Form.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            height: 400px;
            max-height: 50vh;
        }
        .step-item.active {
            transform: scale(1.05);
            box-shadow: 0 0 0 3px #0891b2; /* cyan-600 */
        }
        .nav-link.active {
            color: #0e7490; /* cyan-700 */
            font-weight: 600;
        }
        .tab.active {
            background-color: #0e7490; /* cyan-700 */
            color: white;
        }
        .loader {
            width: 24px;
            height: 24px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #0891b2;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="bg-stone-50 text-slate-800">

    <header class="bg-stone-50/80 backdrop-blur-lg sticky top-0 z-50 border-b border-stone-200">
        <nav class="container mx-auto px-4">
            <div class="flex items-center justify-between h-16">
                <h1 class="text-xl font-bold text-slate-900">RAG Explained</h1>
                <div class="hidden md:flex items-center space-x-6 lg:space-x-8">
                    <a href="#home" class="nav-link text-slate-600 hover:text-cyan-700 transition-colors">Home</a>
                    <a href="#pipeline" class="nav-link text-slate-600 hover:text-cyan-700 transition-colors">The Pipeline</a>
                    <a href="#simulator" class="nav-link text-slate-600 hover:text-cyan-700 transition-colors">RAG Simulator</a>
                    <a href="#comparison" class="nav-link text-slate-600 hover:text-cyan-700 transition-colors">RAG vs. Fine-Tuning</a>
                    <a href="#code" class="nav-link text-slate-600 hover:text-cyan-700 transition-colors">Code Deep-Dive</a>
                    <a href="#quiz" class="nav-link text-slate-600 hover:text-cyan-700 transition-colors">Knowledge Check</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="container mx-auto px-4 py-8 md:py-12">
        
        <section id="home" class="min-h-[80vh] flex items-center">
            <div class="w-full grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                <div>
                    <h2 class="text-4xl md:text-5xl font-bold tracking-tight text-slate-900">Grounding AI in Reality.</h2>
                    <p class="mt-4 text-lg text-slate-600">Retrieval-Augmented Generation (RAG) revolutionizes how Large Language Models (LLMs) work. Instead of just relying on their static, pre-trained memory, RAG systems consult an external knowledge base in real-time. Think of it as an "open-book exam" for AI, ensuring answers are accurate, current, and trustworthy.</p>
                    <p class="mt-4 text-slate-600">This interactive guide, now enhanced with the Gemini API, will walk you through the entire RAG process, from core concepts to a live simulation.</p>
                </div>
                <div class="bg-white p-6 rounded-xl shadow-lg border border-stone-200">
                    <h3 class="font-semibold text-center text-slate-900">LLM Response Generation: A Comparison</h3>
                    <div class="mt-4 grid grid-cols-1 md:grid-cols-2 gap-4 text-sm">
                        <div class="border-r border-stone-200 pr-4">
                            <h4 class="font-semibold text-red-600">Standard LLM (Closed-Book)</h4>
                            <ol class="mt-2 space-y-2 list-inside">
                                <li class="flex items-start"><span class="text-red-600 mr-2">1.</span><span>User asks a question.</span></li>
                                <li class="flex items-start"><span class="text-red-600 mr-2">2.</span><span>LLM searches its vast, static internal memory.</span></li>
                                <li class="flex items-start"><span class="text-red-600 mr-2">3.</span><span>Generates an answer based on pre-trained patterns.</span></li>
                                <li class="mt-2 p-2 bg-red-50 text-red-700 rounded-md text-xs">⚠️ Risk of outdated info or "hallucinations".</li>
                            </ol>
                        </div>
                        <div>
                            <h4 class="font-semibold text-cyan-700">RAG-Powered LLM (Open-Book)</h4>
                            <ol class="mt-2 space-y-2 list-inside">
                               <li class="flex items-start"><span class="text-cyan-700 mr-2">1.</span><span>User asks a question.</span></li>
                               <li class="flex items-start"><span class="text-cyan-700 mr-2">2.</span><span class="font-bold">Retrieves relevant, up-to-date documents.</span></li>
                               <li class="flex items-start"><span class="text-cyan-700 mr-2">3.</span><span>Provides documents + question to LLM.</span></li>
                               <li class="flex items-start"><span class="text-cyan-700 mr-2">4.</span><span>Generates an answer grounded in the provided facts.</span></li>
                               <li class="mt-2 p-2 bg-cyan-50 text-cyan-800 rounded-md text-xs">✅ Answer is verifiable, accurate, and current.</li>
                            </ol>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="pipeline" class="py-16 md:py-24">
            <div class="text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900">The RAG Pipeline: An Interactive Journey</h2>
                <p class="mt-4 max-w-3xl mx-auto text-slate-600">A RAG system works in two main phases. The "Ingestion" phase is a one-time setup to build the knowledge library. The "Retrieval & Generation" phase happens in real-time for every user query. Click on each step below to see how it works and which part of the code handles it.</p>
            </div>
            
            <div id="pipeline-content-display" class="mt-8 bg-white p-6 rounded-xl shadow-lg border border-stone-200 min-h-[200px] transition-all duration-300">
                <p class="text-slate-500 text-center">Click a step in the diagrams below to learn more.</p>
            </div>

            <div class="mt-12 grid grid-cols-1 md:grid-cols-2 gap-12">
                <div>
                    <h3 class="text-xl font-semibold text-center text-slate-900">Phase 1: Ingestion & Indexing (Offline)</h3>
                    <div id="ingestion-steps" class="mt-6 flex flex-col items-center space-y-4"></div>
                </div>
                <div>
                    <h3 class="text-xl font-semibold text-center text-slate-900">Phase 2: Retrieval & Generation (Online)</h3>
                    <div id="retrieval-steps" class="mt-6 flex flex-col items-center space-y-4"></div>
                </div>
            </div>
        </section>
        
        <section id="simulator" class="py-16 md:py-24 bg-white rounded-2xl shadow-lg border border-stone-200">
            <div class="text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900">✨ RAG in Action: The Gemini Simulator</h2>
                <p class="mt-4 max-w-3xl mx-auto text-slate-600">Experience RAG firsthand. Paste a block of text below, ask a question about it, and watch how the Gemini API simulates the RAG process to give you a fact-based answer.</p>
            </div>
            <div class="mt-8 max-w-5xl mx-auto">
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <label for="rag-context" class="font-semibold text-slate-700">1. Provide Your Knowledge Base</label>
                        <textarea id="rag-context" rows="10" class="mt-2 w-full p-3 border border-stone-300 rounded-lg focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500" placeholder="Paste a paragraph or two of text here. For example, a news article, a product description, or a few paragraphs from a report."></textarea>
                    </div>
                    <div>
                        <label for="rag-query" class="font-semibold text-slate-700">2. Ask a Question</label>
                        <input type="text" id="rag-query" class="mt-2 w-full p-3 border border-stone-300 rounded-lg focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500" placeholder="e.g., What was the main finding?">
                        <button id="run-rag-button" class="mt-4 w-full bg-cyan-600 text-white font-semibold px-6 py-3 rounded-lg hover:bg-cyan-700 transition-colors flex items-center justify-center space-x-2">
                            <span>Simulate RAG with Gemini</span>
                        </button>
                    </div>
                </div>
                <div id="rag-results" class="mt-8 grid grid-cols-1 lg:grid-cols-3 gap-6 hidden">
                    <div>
                        <h3 class="font-semibold text-slate-900">Step 1: Chunked Document</h3>
                        <div id="rag-chunks-output" class="mt-2 text-sm space-y-2"></div>
                    </div>
                    <div>
                        <h3 class="font-semibold text-slate-900">Step 2: Retrieved Context</h3>
                        <div id="rag-retrieved-output" class="mt-2 text-sm p-3 bg-cyan-50 border border-cyan-200 rounded-lg min-h-[100px]"></div>
                    </div>
                    <div>
                        <h3 class="font-semibold text-slate-900">Step 3: Generated Answer</h3>
                        <div id="rag-generated-output" class="mt-2 text-sm p-3 bg-green-50 border border-green-200 rounded-lg min-h-[100px]"></div>
                    </div>
                </div>
            </div>
        </section>

        <section id="comparison" class="py-16 md:py-24">
            <div class="text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900">RAG vs. Fine-Tuning</h2>
                <p class="mt-4 max-w-3xl mx-auto text-slate-600">While both techniques adapt LLMs, they solve different problems. RAG provides external knowledge, while Fine-Tuning adjusts the model's internal behavior. Click on the bars in the chart to see a detailed comparison for each criterion.</p>
            </div>
            <div class="mt-8">
                <div class="chart-container">
                    <canvas id="ragVsFinetuningChart"></canvas>
                </div>
                <div id="comparison-explanation" class="mt-8 max-w-3xl mx-auto p-6 bg-stone-100 rounded-lg text-slate-700 min-h-[100px]">
                    <p class="text-slate-500 text-center">Click a category on the chart for details.</p>
                </div>
            </div>
        </section>

        <section id="code" class="py-16 md:py-24">
            <div class="text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900">Code Deep-Dive</h2>
                <p class="mt-4 max-w-3xl mx-auto text-slate-600">Let's explore the Python code that powers our RAG pipeline. Use the tabs to switch between functions, and click the ✨ button to get a dynamic explanation from the Gemini API.</p>
            </div>
            <div class="mt-12 max-w-6xl mx-auto">
                <div class="flex border-b border-stone-200" id="code-tabs"></div>
                <div id="code-content" class="mt-4 bg-slate-900 rounded-b-xl rounded-tr-xl overflow-hidden shadow-2xl"></div>
            </div>
        </section>

        <section id="quiz" class="py-16 md:py-24">
            <div class="max-w-3xl mx-auto text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900">Test Your Knowledge</h2>
                <p class="mt-4 text-slate-600">Ready to check your understanding? Take this short quiz to see if you've mastered the core concepts of RAG.</p>
            </div>
            <div id="quiz-container" class="mt-12 max-w-3xl mx-auto bg-white p-6 md:p-8 rounded-xl shadow-lg border border-stone-200"></div>
        </section>

    </main>
    
    <!-- Gemini Explanation Modal -->
    <div id="explanation-modal" class="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center p-4 z-50 hidden">
        <div class="bg-white rounded-lg shadow-xl max-w-2xl w-full max-h-[90vh] overflow-y-auto">
            <div class="p-6">
                <div class="flex justify-between items-center">
                    <h3 class="text-xl font-bold text-slate-900">✨ Gemini Code Explainer</h3>
                    <button id="close-modal-button" class="text-slate-500 hover:text-slate-800">&times;</button>
                </div>
                <div id="modal-content" class="mt-4 text-slate-700"></div>
            </div>
        </div>
    </div>

    <footer class="bg-slate-900 text-stone-300 py-8">
        <div class="container mx-auto px-4 text-center">
            <p>Interactive RAG Explainer with Gemini</p>
            <p class="text-sm text-stone-400 mt-1">A dynamic guide to understanding Retrieval-Augmented Generation.</p>
        </div>
    </footer>

<script>
document.addEventListener('DOMContentLoaded', () => {

    // --- Data Store ---
    const pipelineData = {
        ingestion: [
            { id: 'load', title: '1. Load Documents', description: 'The process begins by loading raw data from various sources (e.g., text files, PDFs, databases). The goal is to get all potential knowledge into memory.', codeFunction: 'chunking (initial part)' },
            { id: 'chunk', title: '2. Chunk Documents', description: 'Large documents are broken down into smaller, semantically coherent "chunks". This is crucial because LLMs have a limited context window, and smaller chunks lead to more precise retrieval. The code uses a token-based size limit.', codeFunction: 'chunking' },
            { id: 'embed', title: '3. Create Embeddings', description: 'Each text chunk is converted into a numerical vector (an "embedding") using a specialized model. These vectors capture the semantic meaning of the text, allowing for mathematical comparison of concepts.', codeFunction: 'map_document_embeddings' },
            { id: 'index', title: '4. Index & Store', description: 'The embeddings and their corresponding text chunks are stored in a vector database. This database is optimized for fast similarity searches. In our code, a simple JSON file acts as this database.', codeFunction: 'map_document_embeddings' }
        ],
        retrieval: [
            { id: 'query', title: '1. User Query', description: 'The user submits a question or prompt to the system.' },
            { id: 'query-embed', title: '2. Embed Query', description: 'The user\'s query is converted into a vector embedding using the *exact same* model that was used for the documents. This ensures the query and documents are in the same "semantic space".', codeFunction: 'retrieve_information' },
            { id: 'retrieve', title: '3. Retrieve Context', description: 'The system searches the vector database to find the chunks whose embeddings are most similar to the query embedding. This is typically done using Cosine Similarity.', codeFunction: 'retrieve_information' },
            { id: 'augment', title: '4. Augment Prompt', description: 'The retrieved text chunks (the context) are combined with the original user query into a single, detailed prompt for the LLM.', codeFunction: 'generate_llm_response' },
            { id: 'generate', title: '5. Generate Response', description: 'The augmented prompt is sent to the generator LLM (e.g., GPT-3.5). The LLM is instructed to use the provided context to formulate its final, fact-grounded answer.', codeFunction: 'generate_llm_response' }
        ]
    };

    const comparisonData = {
        'Cost': { rag: 6, finetuning: 9, explanation: 'RAG has a lower upfront cost as it avoids expensive model training. Fine-tuning requires significant computational resources (GPUs) and time for training, making it much more expensive to get started.' },
        'Data Freshness': { rag: 10, finetuning: 3, explanation: 'RAG excels here. Its knowledge is dynamic and can be updated in real-time by simply changing the external data source. A fine-tuned model\'s knowledge is static and frozen at the time of training; updates require a full, costly retraining cycle.' },
        'Hallucination Risk': { rag: 3, finetuning: 7, explanation: 'RAG significantly reduces hallucinations by grounding the LLM in verifiable, retrieved facts. Fine-tuning can still lead to hallucinations as the model relies on its internalized (and potentially flawed) patterns.' },
        'Transparency': { rag: 9, finetuning: 2, explanation: 'RAG offers high transparency. It can cite the exact sources used to generate an answer, allowing for verification. Fine-tuning is a "black box"; it\'s impossible to trace why the model generated a specific response.' },
        'Task Type': { rag: 8, finetuning: 5, explanation: 'RAG is best for knowledge-intensive tasks requiring factual, up-to-date information (e.g., Q&A bots). Fine-tuning is best for teaching the model a new skill, style, or format (e.g., adopting a brand voice, generating structured JSON).' }
    };

    const codeData = {
        chunking: {
            title: 'chunking()',
            code: `def chunking(directory_path, tokenizer, chunk_size, ...):
    documents = {}
    all_chunks = {}
    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        if os.path.isfile(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
            
            doc_id = str(uuid.uuid4())
            paragraphs = re.split(para_seperator, text)
            
            for paragraph in paragraphs:
                words = paragraph.split(separator)
                current_chunk_str = ""
                # ... chunking logic ...
    return documents`,
            explanation: `This function reads text files, splits them into smaller chunks based on a token limit, and organizes them.`
        },
        map_document_embeddings: {
            title: 'map_document_embeddings()',
            code: `def map_document_embeddings(documents, tokenizer, model):
    mapped_document_db = {}
    for id, dict_content in documents.items():
        for content_id, text_content in dict_content.items():
            text = text_content.get("text")
            inputs = tokenizer(text, return_tensors="pt", ...)
            with torch.no_grad():
                outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
            mapped_embeddings[content_id] = embeddings.squeeze().tolist()
    return mapped_document_db`,
            explanation: `This converts every text chunk into a numerical vector (embedding) using a pre-trained model.`
        },
        retrieve_information: {
            title: 'retrieve_top_k_scores()',
            code: `def retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k):
    query_embeddings = np.array(query_embeddings)
    scores = {}
    for doc_id, chunk_dict in mapped_document_db.items():
        for chunk_id, chunk_embeddings in chunk_dict.items():
            chunk_embeddings = np.array(chunk_embeddings)
            # Calculate Cosine Similarity
            score = ... 
            scores[(doc_id, chunk_id)] = score
            
    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)
    return sorted_scores[:top_k]`,
            explanation: `This function calculates the cosine similarity between the user's query embedding and all chunk embeddings to find the most relevant chunks.`
        },
        generate_llm_response: {
            title: 'generate_llm_response()',
            code: `def generate_llm_response(openai_model, query, relevant_text):
    template = """
    You are an intelligent search engine. Answer based on the
    retrieved context.
    Context: {context}
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template=template)
    chain = prompt | openai_model
    response = chain.invoke({
        "context": relevant_text["text"], 
        "question": query
    })
    return response`,
            explanation: `This takes the retrieved context and the original query, formats them into a detailed prompt, and sends it to the generator LLM.`
        }
    };

    const quizData = [
        { question: "What is the primary problem RAG is designed to solve?", options: ["Slow inference speed", "Static knowledge and hallucinations", "Inability to understand grammar", "Difficulty generating creative text"], answer: "Static knowledge and hallucinations", explanation: "RAG's main goal is to combat factual inaccuracies (hallucinations) and the outdated knowledge of LLMs by grounding them in external, verifiable data." },
        { question: "In the RAG pipeline, what is 'chunking'?", options: ["Summarizing documents", "Encrypting documents", "Breaking large documents into smaller pieces", "Translating documents"], answer: "Breaking large documents into smaller pieces", explanation: "Chunking breaks down large texts into smaller, manageable pieces to ensure the retrieved context is specific and fits within the LLM's context window." },
        { question: "Why must the same embedding model be used for documents and the user query?", options: ["It's a licensing requirement", "To reduce memory usage", "To ensure they are in the same comparable 'semantic space'", "Any model from the same company works"], answer: "To ensure they are in the same comparable 'semantic space'", explanation: "Using the same model places the query and document vectors in the same coordinate system, making meaningful similarity comparisons possible." },
        { question: "When is Fine-Tuning a better choice than RAG?", options: ["When you need to answer questions about today's news", "When you need to cite sources for every answer", "When you need to teach the LLM a specific style or skill", "When you have a very limited budget"], answer: "When you need to teach the LLM a specific style or skill", explanation: "Fine-tuning is ideal for adapting the model's inherent behavior, such as its writing style, tone, or ability to generate structured output like JSON, rather than injecting factual knowledge." },
        { question: "What does Cosine Similarity measure?", options: ["The distance between two vectors", "The difference in length of two vectors", "The similarity in orientation (direction) of two vectors", "The number of shared keywords"], answer: "The similarity in orientation (direction) of two vectors", explanation: "Cosine similarity measures the cosine of the angle between two vectors. A smaller angle (closer to 0°) means higher similarity (a score closer to 1)." },
        { question: "What is the main role of a vector database in a RAG system?", options: ["To store user login information", "To store and efficiently search vector embeddings", "To store the original text documents in a compressed format", "To train the language model"], answer: "To store and efficiently search vector embeddings", explanation: "Vector databases are specialized to store vector embeddings and perform incredibly fast similarity searches, which is the core of the retrieval step." },
        { question: "In the context of the `generate_llm_response` function, what is a 'prompt template'?", options: ["A pre-written final answer", "A user interface for typing questions", "A text structure with placeholders for context and a query", "A type of language model"], answer: "A text structure with placeholders for context and a query", explanation: "A prompt template is a blueprint for the final prompt, containing instructions and placeholders (like {context} and {question}) that get filled in at runtime." },
        { question: "What does 'mean pooling' achieve when creating embeddings from text chunks?", options: ["It finds the most important word", "It makes the embedding shorter to save space", "It averages token embeddings to create a single vector for the whole chunk", "It encrypts the embedding vector"], answer: "It averages token embeddings to create a single vector for the whole chunk", explanation: "Mean pooling is a strategy to create a single, fixed-size 'sentence embedding' that represents the entire chunk's meaning by averaging the embeddings of all its tokens." },
        { question: "Why is a simple JSON file not suitable as a vector store for a large-scale, production RAG system?", options: ["JSON files cannot store numbers", "It is not scalable and lacks efficient search algorithms", "JSON is a proprietary format", "It is too slow to read and write any file"], answer: "It is not scalable and lacks efficient search algorithms", explanation: "Production systems need specialized vector databases that use algorithms like Approximate Nearest Neighbor (ANN) search to find results quickly in massive datasets, which is not possible by iterating through a JSON file." },
        { question: "A 'hybrid approach' to RAG can involve combining...", options: ["Multiple different programming languages", "RAG with fine-tuning, or semantic search with keyword search", "A chatbot with a search engine", "Using both a CPU and a GPU"], answer: "RAG with fine-tuning, or semantic search with keyword search", explanation: "The term 'hybrid' in RAG often refers to combining techniques, like using a fine-tuned model as the generator in a RAG pipeline or using both semantic (vector) and keyword search for more robust retrieval." }
    ];

    // --- DOM Elements ---
    const navLinks = document.querySelectorAll('.nav-link');
    const sections = document.querySelectorAll('main section');
    const pipelineContentDisplay = document.getElementById('pipeline-content-display');
    const ingestionStepsContainer = document.getElementById('ingestion-steps');
    const retrievalStepsContainer = document.getElementById('retrieval-steps');
    const comparisonExplanation = document.getElementById('comparison-explanation');
    const codeTabsContainer = document.getElementById('code-tabs');
    const codeContentContainer = document.getElementById('code-content');
    const quizContainer = document.getElementById('quiz-container');
    const runRagButton = document.getElementById('run-rag-button');
    const ragResultsContainer = document.getElementById('rag-results');
    const ragChunksOutput = document.getElementById('rag-chunks-output');
    const ragRetrievedOutput = document.getElementById('rag-retrieved-output');
    const ragGeneratedOutput = document.getElementById('rag-generated-output');
    const explanationModal = document.getElementById('explanation-modal');
    const modalContent = document.getElementById('modal-content');
    const closeModalButton = document.getElementById('close-modal-button');

    // --- Gemini API Helper ---
    async function callGemini(prompt) {
        const apiKey = ""; 
        const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;
        
        const payload = {
            contents: [{ role: "user", parts: [{ text: prompt }] }]
        };

        try {
            const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });

            if (!response.ok) {
                const errorBody = await response.text();
                throw new Error(`API Error: ${response.status} ${response.statusText} - ${errorBody}`);
            }

            const result = await response.json();
            if (result.candidates && result.candidates.length > 0 && result.candidates[0].content.parts.length > 0) {
                return result.candidates[0].content.parts[0].text;
            } else {
                return "Sorry, I couldn't get a response. The response format was unexpected.";
            }
        } catch (error) {
            console.error("Gemini API call failed:", error);
            return `An error occurred: ${error.message}. Please check the console for details.`;
        }
    }

    // --- Initial Population ---
    function populatePipeline() {
        const createStepHTML = (step, index, total) => `
            <div class="w-full">
                <div data-id="${step.id}" class="step-item bg-white p-4 rounded-lg shadow border border-stone-200 cursor-pointer hover:shadow-md hover:border-cyan-500 transition-all duration-200">
                    <p class="font-semibold text-slate-900">${step.title}</p>
                </div>
                ${index < total - 1 ? '<div class="h-8 w-px bg-stone-300 mx-auto"></div>' : ''}
            </div>`;
        ingestionStepsContainer.innerHTML = pipelineData.ingestion.map((step, i) => createStepHTML(step, i, pipelineData.ingestion.length)).join('');
        retrievalStepsContainer.innerHTML = pipelineData.retrieval.map((step, i) => createStepHTML(step, i, pipelineData.retrieval.length)).join('');
    }

    function populateCode() {
        codeTabsContainer.innerHTML = Object.keys(codeData).map((key, index) => `
            <button class="tab px-4 py-2 text-sm font-medium text-slate-500 hover:bg-slate-100 rounded-t-lg ${index === 0 ? 'active' : ''}" data-tab="${key}">${codeData[key].title}</button>
        `).join('');

        codeContentContainer.innerHTML = Object.keys(codeData).map((key, index) => `
            <div id="code-${key}" class="code-pane p-4 md:p-6 ${index === 0 ? '' : 'hidden'}">
                <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
                    <div>
                        <pre class="bg-slate-800 p-4 rounded-lg overflow-x-auto"><code class="language-python text-sm text-stone-200">${codeData[key].code}</code></pre>
                        <button class="explain-code-button mt-4 bg-sky-600 text-white text-sm font-semibold px-4 py-2 rounded-lg hover:bg-sky-700 transition-colors flex items-center justify-center space-x-2" data-code-key="${key}">
                            <span>✨ Explain with Gemini</span>
                        </button>
                    </div>
                    <div class="text-stone-300">${codeData[key].explanation}</div>
                </div>
            </div>
        `).join('');
    }
    
    function populateQuiz() {
        let quizHTML = '<form id="quiz-form">';
        quizData.forEach((q, index) => {
            quizHTML += `
                <div class="mb-6">
                    <p class="font-semibold mb-2">${index + 1}. ${q.question}</p>
                    <div class="space-y-2">
                        ${q.options.map(option => `
                            <div>
                                <label class="flex items-center p-3 rounded-lg border border-stone-200 hover:bg-stone-100 cursor-pointer">
                                    <input type="radio" name="question${index}" value="${option}" class="mr-3 h-4 w-4 text-cyan-600 focus:ring-cyan-500 border-gray-300">
                                    <span>${option}</span>
                                </label>
                            </div>
                        `).join('')}
                    </div>
                    <div id="feedback${index}" class="mt-2 text-sm p-3 rounded-lg hidden"></div>
                </div>
            `;
        });
        quizHTML += `
            <div class="flex justify-between items-center">
                <button type="submit" class="bg-cyan-600 text-white font-semibold px-6 py-2 rounded-lg hover:bg-cyan-700 transition-colors">Submit Answers</button>
                <div id="quiz-score" class="font-semibold text-lg"></div>
            </div>
        </form>`;
        quizContainer.innerHTML = quizHTML;
    }

    // --- Chart.js Implementation ---
    function createComparisonChart() {
        const ctx = document.getElementById('ragVsFinetuningChart').getContext('2d');
        const labels = Object.keys(comparisonData);
        new Chart(ctx, {
            type: 'bar',
            data: {
                labels: labels,
                datasets: [
                    { label: 'RAG', data: labels.map(l => comparisonData[l].rag), backgroundColor: 'rgba(14, 116, 144, 0.7)' },
                    { label: 'Fine-Tuning', data: labels.map(l => comparisonData[l].finetuning), backgroundColor: 'rgba(244, 63, 94, 0.7)' }
                ]
            },
            options: {
                responsive: true, maintainAspectRatio: false,
                scales: { y: { beginAtZero: true, max: 10, title: { display: true, text: 'Relative Score (Higher is Better)' } } },
                plugins: { legend: { position: 'top' } },
                onClick: (event, elements) => {
                    if (elements.length > 0) {
                        const label = event.chart.data.labels[elements[0].index];
                        updateComparisonExplanation(label);
                    }
                }
            }
        });
    }

    // --- Event Handlers & Logic ---
    function handlePipelineClick(e) {
        const stepElement = e.target.closest('.step-item');
        if (!stepElement) return;
        const stepId = stepElement.dataset.id;
        const stepInfo = [...pipelineData.ingestion, ...pipelineData.retrieval].find(s => s.id === stepId);
        if (stepInfo) {
            pipelineContentDisplay.innerHTML = `<h4 class="font-bold text-lg text-slate-900">${stepInfo.title}</h4><p class="mt-2 text-slate-600">${stepInfo.description}</p>${stepInfo.codeFunction ? `<p class="mt-3 text-sm text-cyan-700 font-medium">Handled by: code>${stepInfo.codeFunction}</code></p>` : ''}`;
            document.querySelectorAll('.step-item').forEach(el => el.classList.remove('active'));
            stepElement.classList.add('active');
        }
    }

    function updateComparisonExplanation(label) {
        const data = comparisonData[label];
        if (data) comparisonExplanation.innerHTML = `<h4 class="font-bold text-lg text-slate-900">${label}</h4><p class="mt-2 text-slate-600">${data.explanation}</p>`;
    }

    function handleCodeTabClick(e) {
        const tabButton = e.target.closest('.tab');
        if (!tabButton) return;
        const tabId = tabButton.dataset.tab;
        document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
        tabButton.classList.add('active');
        document.querySelectorAll('.code-pane').forEach(pane => pane.classList.add('hidden'));
        document.getElementById(`code-${tabId}`).classList.remove('hidden');
    }

    function handleQuizSubmit(e) {
        e.preventDefault();
        let score = 0;
        quizData.forEach((q, index) => {
            const selectedOption = e.target.querySelector(`input[name="question${index}"]:checked`);
            const feedbackEl = document.getElementById(`feedback${index}`);
            feedbackEl.classList.remove('hidden', 'bg-green-100', 'text-green-800', 'bg-red-100', 'text-red-800');
            if (selectedOption) {
                if (selectedOption.value === q.answer) {
                    score++;
                    feedbackEl.innerHTML = `<strong>Correct!</strong> ${q.explanation}`;
                    feedbackEl.classList.add('bg-green-100', 'text-green-800');
                } else {
                    feedbackEl.innerHTML = `<strong>Incorrect.</strong> The correct answer is "${q.answer}". ${q.explanation}`;
                    feedbackEl.classList.add('bg-red-100', 'text-red-800');
                }
            } else {
                feedbackEl.innerHTML = 'Please select an answer.';
                feedbackEl.classList.add('bg-yellow-100', 'text-yellow-800');
            }
        });
        document.getElementById('quiz-score').textContent = `Your Score: ${score} / ${quizData.length}`;
    }

    function handleScroll() {
        let currentSectionId = '';
        sections.forEach(section => {
            if (pageYOffset >= section.offsetTop - 80) {
                currentSectionId = section.id;
            }
        });
        navLinks.forEach(link => {
            link.classList.toggle('active', link.hash === `#${currentSectionId}`);
        });
    }

    async function handleRagSimulation() {
        const context = document.getElementById('rag-context').value;
        const query = document.getElementById('rag-query').value;

        if (!context.trim() || !query.trim()) {
            alert("Please provide both a knowledge base and a question.");
            return;
        }

        runRagButton.disabled = true;
        runRagButton.innerHTML = `<div class="loader"></div><span class="ml-2">Simulating...</span>`;
        ragResultsContainer.classList.remove('hidden');
        ragChunksOutput.innerHTML = `<div class="flex items-center justify-center p-4"><div class="loader"></div></div>`;
        ragRetrievedOutput.innerHTML = `<div class="flex items-center justify-center p-4"><div class="loader"></div></div>`;
        ragGeneratedOutput.innerHTML = `<div class="flex items-center justify-center p-4"><div class="loader"></div></div>`;

        // Step 1: Chunking (simple split by newline for simulation)
        const chunks = context.split('\n').filter(c => c.trim() !== '');
        ragChunksOutput.innerHTML = chunks.map((chunk, i) => `<div class="p-2 border rounded-md bg-stone-50">${i + 1}. ${chunk}</div>`).join('');

        // Step 2: Retrieve Context (use Gemini to find the most relevant chunk)
        const retrievalPrompt = `Given the following document chunks and a user query, identify the single most relevant chunk to answer the query. Respond with only the text of that chunk.

DOCUMENT CHUNKS:
---
${chunks.join('\n---\n')}
---

USER QUERY:
"${query}"

MOST RELEVANT CHUNK:`;
        const retrievedChunk = await callGemini(retrievalPrompt);
        ragRetrievedOutput.textContent = retrievedChunk;

        // Step 3: Generate Response
        const generationPrompt = `You are an intelligent assistant. Answer the following question based ONLY on the provided context. If the context does not contain the answer, say "The provided context does not contain enough information to answer this question."

CONTEXT:
"${retrievedChunk}"

QUESTION:
"${query}"

ANSWER:`;
        const generatedAnswer = await callGemini(generationPrompt);
        ragGeneratedOutput.textContent = generatedAnswer;

        runRagButton.disabled = false;
        runRagButton.innerHTML = `<span>Simulate RAG with Gemini</span>`;
    }
    
    async function handleExplainCode(e) {
        const button = e.target.closest('.explain-code-button');
        if (!button) return;
        
        const codeKey = button.dataset.codeKey;
        const codeToExplain = codeData[codeKey].code;
        
        modalContent.innerHTML = `<div class="flex items-center justify-center p-8"><div class="loader"></div></div>`;
        explanationModal.classList.remove('hidden');

        const prompt = `You are an expert Python programmer and teacher. Explain the following Python code snippet line-by-line in a way that is easy for a beginner to understand. Use markdown for formatting.

\`\`\`python
${codeToExplain}
\`\`\``;

        const explanation = await callGemini(prompt);
        // Basic markdown to HTML
        let formattedExplanation = explanation
            .replace(/```python\n/g, '<pre class="bg-slate-800 text-white p-3 rounded-md text-sm"><code>')
            .replace(/```/g, '</code></pre>')
            .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
            .replace(/`(.*?)`/g, '<code class="bg-stone-200 text-sm px-1 rounded">$1</code>')
            .replace(/\n/g, '<br>');

        modalContent.innerHTML = formattedExplanation;
    }

    // --- Attach Event Listeners ---
    window.addEventListener('scroll', handleScroll);
    ingestionStepsContainer.addEventListener('click', handlePipelineClick);
    retrievalStepsContainer.addEventListener('click', handlePipelineClick);
    codeTabsContainer.addEventListener('click', handleCodeTabClick);
    codeContentContainer.addEventListener('click', handleExplainCode);
    quizContainer.addEventListener('submit', handleQuizSubmit);
    runRagButton.addEventListener('click', handleRagSimulation);
    closeModalButton.addEventListener('click', () => explanationModal.classList.add('hidden'));

    // --- Run Initializers ---
    populatePipeline();
    createComparisonChart();
    populateCode();
    populateQuiz();
    handleScroll();
});
</script>
</body>
</html>
